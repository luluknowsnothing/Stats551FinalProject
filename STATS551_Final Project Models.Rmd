---
title: "STATS551 Final Project Models"
date: "3/22/2021"
output: 
  html_document
    toc: true # show table of content
    toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
    toc_float: 
      collapsed: false # float the table of contents to the left
    number_sections: true  # number sections at each table header
    theme: united  # theme option
    highlight: tango  # specifies the syntax highlighting style
---

```{r setup, include=FALSE}
# setup
knitr::opts_chunk$set(echo = FALSE, fig.width = 6, fig.height = 3)
library(knitr)
library(tidyverse)
library(xlsx)
library(lmerTest)
library(emmeans)
library(forecast)
```

```{r functions, include=FALSE}
# functions to get key variables

# parameters required: 
# dataframe: original data in the format of data frame, where nrow = samplesize
# answertable: a m*n table of chosen answer for each response, 
##             where m is sample size and n is number of questions. 
# correctanswervector: a n-element vector of correct answers for n items.
# belieflist: a list of assigned beliefs, list length = questionlength, 
# #           list[i] is a nrow*ncol matrix of beliefs for question i, 
##            nrow = samplesize, ncol= number of options for question i.  


# Samplesize of the dataset
samplesize <- function(dataframe){
  size<-nrow(dataframe)
  print(paste0('The dataset has SampleSize = ',size))
  return(size)
}

# CorrectCheck: check the correctness of every response in an answertable, where 1 means correct and 0 means wrong.
correctcheck <- function(answertable,correctanswervector){
  row = nrow(answertable)
  col= ncol(answertable)
  check<-matrix(rep(NA,row*col),
                nrow=row,ncol=col,
                byrow=TRUE)
  # let m be the row index and n be the column index of matrix check
  for (m in 1:row){
   for (n in 1:col){
    check[m,n]<- ifelse(answertable[m,n]==correctanswervector[n],1,0)
   }
  }
  return(check)
}

# correctbelief: a table of beliefs assigned to the correct answers, samplesize*questionsize.
correctbelief<-function(belieflist, correctanswervector){
  samplesize<-nrow(belieflist[[1]])
  questionsize<-length(belieflist)
  Correctbelief<-matrix(rep(NA,samplesize*questionsize),
                nrow=samplesize,ncol=questionsize,
                byrow=TRUE)
  for (n in 1: questionsize){
    for (m in 1: samplesize){
      correctchoice <- correctanswervector[n]
      Correctbelief[m,n]<-belieflist[[n]][m,correctchoice]
    }
  }
  return(Correctbelief)
  
}

# chosenbelief: a table of beliefs assigned to the chosen answer, samplesize*questionsize
chosenbelief<-function(belieflist,answertable){
  samplesize<-nrow(answertable)
  questionsize<-ncol(answertable)
  chosenbelief = matrix(rep(NA,samplesize*questionsize),
                    nrow=samplesize,ncol=questionsize)
  for (n in 1: questionsize){
    for (m in 1: samplesize){
      choice <- answertable[m,n]
      chosenbelief[m,n]<-belieflist[[n]][m,choice]
    }
  }
  return(chosenbelief)
}


# maxbelief: a table of maximum beliefs in each belief assignment for a question
maxbelief<-function(belieflist){
  samplesize<-nrow(belieflist[[1]])
  questionsize<-length(belieflist)
  maxbelief = matrix(rep(NA,samplesize*questionsize),
                    nrow=samplesize,ncol=questionsize)
  for (n in 1: questionsize){
    for (m in 1: samplesize){
      maxbelief[m,n]<-max(belieflist[[n]][m,])
    }
  }
  return(maxbelief)
}

# Mismatch is matrix, recording the number of mismatch each participants have. If mismatch>=25% of total, the participant would be considered as inattentive and excluded from analysis.
mismatch<-function(maxbelief,chosenbelief){
  samplesize<-nrow(chosenbelief)
  questionsize<-ncol(chosenbelief)  
  MismatchTable<- matrix(rep(NA,samplesize*questionsize),
                    nrow=samplesize,ncol=questionsize)
  for (i in 1:samplesize) {
    for (j in 1:questionsize) {
      ifelse(chosenbelief[i,j]!=maxbelief[i,j],MismatchTable[i,j]<-1,MismatchTable[i,j]<-0)
    }
  }
  Mismatch<-rowSums(MismatchTable)
  exclude<- c();
  for (i in 1:samplesize) {
    if (Mismatch[i] >= questionsize/4){
      exclude<-c(exclude,i)
    }
  }
  print(paste0('participants have 25% or more mismatch that need to be excluded: ',exclude))
  return(list(matrix(Mismatch,nrow=1,ncol=samplesize,byrow=TRUE),exclude))
}

#-----------------------------------------------------------------#
# accuracy: binary coding (1 or 0) of correct vs. wrong
accuracy <- function(correctcheck){
  samplesize <- nrow(correctcheck)
  questionsize <- ncol(correctcheck)
  optionscore<- c(rep(0,samplesize))
  for (m in 1:samplesize){
   for (n in 1:questionsize){
    optionscore[m]<- optionscore[m]+correctcheck[m,n]
    }
  } 
  accuracy <- optionscore/questionsize
  return(accuracy)
}

# belief accuracy: probabilistic coding of correctness
beliefaccuracy<-function(correctbelief){
  questionsize<-ncol(correctbelief)
  beliefaccuracy<- rowSums(correctbelief)/(questionsize)
  return(beliefaccuracy)
}

# itemwise confidence, or just confidence, the mean belief assigned to the chosen options.
itemconfidence<-function(chosenbelief){
  questionsize<-ncol(chosenbelief)
  itemconfidence<-rowSums(chosenbelief)/(questionsize)
  return(itemconfidence)
}

# Accuracy Percentile: performance percentile calculated by binary score
accuracypercentile<-function(accuracy){
  samplesize<-length(accuracy)
  Apercent<-sapply(1:samplesize, function(x) 100*(rank(accuracy)[x]-1)/samplesize)
  return(Apercent/100)
}

# Belief Accuracy Percentile: performance percentile calculated by probabilistic score
beliefaccuracypercentile<-function(beliefaccuracy){
  samplesize<-length(beliefaccuracy)
  BApercent<-sapply(1:samplesize, function(x) 100*(rank(beliefaccuracy)[x]-1)/samplesize)
  return(BApercent/100)
}


# basic stats for variables
basicssum<-function(variable){
  library(moments)
  vname<-deparse(substitute(variable))
  print(paste0('The variable ', vname, ' has a mean of ', mean(variable,na.rm=TRUE), 
               ', a median of ', median(variable,na.rm=TRUE),
               ', with sd of ', sd(variable,na.rm=TRUE),
               '; a range of ', min(variable,na.rm=TRUE),' to ', max(variable,na.rm=TRUE),
               '; a length of ', length(variable),
               '; kurtosis is ', kurtosis(variable,na.rm=TRUE), 
               '; skewness is ', skewness(variable,na.rm=TRUE)))
  hist(variable, main= vname, xlab= vname)
}

```

# Data Set Clean Up
      
## Dataset 1: Health Care Policy (ACA)
     
```{r, echo=FALSE, cache=TRUE}
# Data Entry
Mydataf <- read.xlsx("ACAData.xlsx",1,header=TRUE, to.data.frame = TRUE)

#----------------------------------------------------------------------#
#----------------------------------------------------------------------#

# PART II A: Data exclusion
# Correct Answers of the questionnaire
correctanswervector <- c(2,2,1,2,1,1,2,2,1,1,1,1,1,2,1,2,2,2) # correct answers

# AnswerTable:a SampleSize*18 table contains the chosen options for each quetion and each participant
answertable <- cbind(Mydataf$Q1,Mydataf$Q2,Mydataf$Q3,Mydataf$Q4,
                     Mydataf$Q5,Mydataf$Q6,Mydataf$Q7,Mydataf$Q8,
                     Mydataf$Q9,Mydataf$Q10,Mydataf$Q11,Mydataf$Q12,
                     Mydataf$Q13,Mydataf$Q14,Mydataf$Q15,Mydataf$Q16,
                     Mydataf$Q17,Mydataf$Q18)

# make sure the answers are in numeric format
axh <- matrix(rep(0,nrow(answertable)*ncol(answertable)),nrow=nrow(answertable),ncol=ncol(answertable))
for (i in 1: nrow(answertable)){
  for (j in 1:ncol(answertable)){
    a <- answertable[i,j]
    axh[i,j] <- as.numeric(a)
  }
}
answertable <- axh

# Belieflist, each list element contains a samplesize*optionsize table recording beliefs
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2)/100
Belief13<- cbind(Mydataf$Q13B1,Mydataf$Q11B2)/100
Belief14<- cbind(Mydataf$Q14B1,Mydataf$Q14B2)/100
Belief15<- cbind(Mydataf$Q15B1,Mydataf$Q15B2)/100
Belief16<- cbind(Mydataf$Q16B1,Mydataf$Q16B2)/100
Belief17<- cbind(Mydataf$Q17B1,Mydataf$Q17B2)/100
Belief18<- cbind(Mydataf$Q18B1,Mydataf$Q18B2)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12,Belief13,Belief14,Belief15,Belief16,Belief17,Belief18)


# Data exclusion: exclude inattentive participants. If the chosen choice has not the maximum probability assigned, this counts as a mismatch. If a participant has equal or more than 25% of answers appear to be mismatch, this participant is regarded as an inattentive participant, and will be excluded from analysis.
ChosenBelief<-chosenbelief(belieflist,answertable)
MaximumBelief<-maxbelief(belieflist)
exclude<-mismatch(MaximumBelief,ChosenBelief)

#----------------------------------------------------------------------#

# PART II B: Data Management and Important Variable Set up
## Mydataf after Excluding SubNum 97-101 (who failed to finish the questionnaire) and inattentive participants: 8(6),12(4),29(8),30(8),54(4),64(4)
Mydataf<-Mydataf[-exclude[[2]],]
# answertable after exlucsion:
answertable<-answertable[-exclude[[2]],]
# belieflist after exclusion: (Mydataf is overrided)
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2)/100
Belief13<- cbind(Mydataf$Q13B1,Mydataf$Q11B2)/100
Belief14<- cbind(Mydataf$Q14B1,Mydataf$Q14B2)/100
Belief15<- cbind(Mydataf$Q15B1,Mydataf$Q15B2)/100
Belief16<- cbind(Mydataf$Q16B1,Mydataf$Q16B2)/100
Belief17<- cbind(Mydataf$Q17B1,Mydataf$Q17B2)/100
Belief18<- cbind(Mydataf$Q18B1,Mydataf$Q18B2)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12,Belief13,Belief14,Belief15,Belief16,Belief17,Belief18)
# chosenbelief after exclusion
ChosenBelief<-ChosenBelief[-exclude[[2]],]
# maximumBelief after exclusion
MaximumBelief<-MaximumBelief[-exclude[[2]],]
# transform correctness of each response into binary values where 1 means correct and 0 means wrong.
CorrectCheck<-correctcheck(answertable,correctanswervector)
# beliefs assigned to the correct choices of each question each participant
CorrectBelief<-correctbelief(belieflist,correctanswervector)

# KEY VARIABLES
SampleSize<-samplesize(Mydataf) # samplesize
Accuracy<-accuracy(CorrectCheck) # accuracy
BeliefAccuracy<-beliefaccuracy(CorrectBelief) # beliefaccuracy
Confidence<-itemconfidence(ChosenBelief) # item-wise confidence
QuestionSize <- ncol(CorrectCheck) # number of questions
OverallConfidence<-Mydataf$EstimatedNumCorrect/QuestionSize # overall confidence (global confidence)
#----------------------------------------------------------------------#
#----------------------------------------------------------------------#
## Beliefs in Social Comparison
SocialBelief1<- cbind(Mydataf$Q1O1,Mydataf$Q1O2)/100
SocialBelief2<- cbind(Mydataf$Q2O1,Mydataf$Q2O2)/100
SocialBelief3<- cbind(Mydataf$Q3O1,Mydataf$Q3O2)/100
SocialBelief4<- cbind(Mydataf$Q4O1,Mydataf$Q4O2)/100
SocialBelief5<- cbind(Mydataf$Q5O1,Mydataf$Q5O2)/100
SocialBelief6<- cbind(Mydataf$Q6O1,Mydataf$Q6O2)/100
SocialBelief7<- cbind(Mydataf$Q7O1,Mydataf$Q7O2)/100
SocialBelief8<- cbind(Mydataf$Q8O1,Mydataf$Q8O2)/100
SocialBelief9<- cbind(Mydataf$Q9O1,Mydataf$Q9O2)/100
SocialBelief10<-cbind(Mydataf$Q10O1,Mydataf$Q10O2)/100
SocialBelief11<- cbind(Mydataf$Q11O1,Mydataf$Q11O2)/100
SocialBelief12<- cbind(Mydataf$Q12O1,Mydataf$Q12O2)/100
SocialBelief13<- cbind(Mydataf$Q13O1,Mydataf$Q13O2)/100
SocialBelief14<- cbind(Mydataf$Q14O1,Mydataf$Q14O2)/100
SocialBelief15<- cbind(Mydataf$Q15O1,Mydataf$Q15O2)/100
SocialBelief16<- cbind(Mydataf$Q16O1,Mydataf$Q16O2)/100
SocialBelief17<- cbind(Mydataf$Q17O1,Mydataf$Q17O2)/100
SocialBelief18<- cbind(Mydataf$Q18O1,Mydataf$Q18O2)/100
socialbelieflist<-list(SocialBelief1,SocialBelief2,SocialBelief3,SocialBelief4,SocialBelief5,SocialBelief6,SocialBelief7,SocialBelief8,SocialBelief9,SocialBelief10,SocialBelief11,SocialBelief12,SocialBelief13,SocialBelief14,SocialBelief15,SocialBelief16,SocialBelief17,SocialBelief18)

# beliefs about other people choosing the correct choice for each question each participant
SocialCorrectBelief<-correctbelief(socialbelieflist,correctanswervector)
# beliefs about other people choosing the choice the participant chose for each question each participant
SocialChosenBelief<-chosenbelief(socialbelieflist,answertable)
# key variables
AccuracyPercentile<-accuracypercentile(Accuracy) # actual performance percentile by binary score
BeliefaccuracyPercentile<-beliefaccuracypercentile(BeliefAccuracy) # actual performance percentile by probabilistic score
SharedBelief <- beliefaccuracy(SocialChosenBelief) # mean beliefs in other people choosing the same answer as I did
SpecificPercentile<-Mydataf$SpecificPercentile/100 # estimate of one's performance relative to others in percentile on the specific questionnaire

```

```{r}
#create data array
data.array = array(dim=c(QuestionSize,3,SampleSize))
for (sub in 1: SampleSize){
  data.array[,,sub]=cbind(SocialChosenBelief[sub,],ChosenBelief[sub,],rep(SpecificPercentile[sub],QuestionSize))
}
# name the array
dimnames(data.array) <- list(1:QuestionSize, c("Other", "Self", "Percentile"), 1:SampleSize)

excludelist <- c()
# exclude inattentive participants whose answers are all the same
for (sub in 1: SampleSize) {
  x=data.array[,,sub][,"Self"]
  y=data.array[,,sub][,"Other"]
  if(length(unique(x))==1||length(unique(y))==1||cor.test(x,y)[["estimate"]]>0.999){
    excludelist = c(excludelist,sub)
  }
}
# excluded data array
data.array<-data.array[,,-excludelist]
SampleSize <- dim(data.array)[3]

data.array.tm <- NULL
for(sub in 1:SampleSize){
  data.array.tm <- rbind(data.array.tm,cbind(data.array[,,sub],Subject=rep(sub,QuestionSize)))
}

# Final Data to enter the model
comb_data= data.frame(Q=rep(1:QuestionSize,SampleSize),data.array.tm)
print(paste0("Final Sample Size is: ",SampleSize))

```
          

### Model theta on Beta(slope) as well         
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda

# setting priors: (for variables after centering)
# baseline ~ N(0,0.2^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2

# together we have a total of 3+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4] to param[3+SampleSize]: baseline
# param[4+SampleSize] to param[3+2*SampleSize]: beta
# param[4+2*SampleSize] to param[3+3*SampleSize]: sdb 

# Likelihood
# Likelihood for Level 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p)
# Likelihood for Level 2: p(beta_p|everything else) = p(beta_p|alpha0,alpha1,sda)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(4+SampleSize):(3+2*SampleSize)] #{beta_p}, vector
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)] #{sdb_p}, vector
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)]
  beta = param[(4+SampleSize):(3+2*SampleSize)]
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)]
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0.5, sd=0.2, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(3+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,
                       rep(0.0045,SampleSize),
                       rep(0.0045,SampleSize),
                       rep(0.004,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,3+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%1000==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.5,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 40000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```


### Model theta on Baseline as well
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
#          baseline(p) ~ r0 + r1*Percentile(p) + N(N,sdr^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda, r0, r1, sdr

# setting priors: (for variables after centering)
# baseline ~ N(0,0.4^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# r0 ~ N(0,0.4^2)
# r1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2
# sdr ~ beta(2,5)

# together we have a total of 6+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4]: r0;
# param[5]: r1;
# param[6]: sdr;
# param[7] to param[6+SampleSize]: baseline
# param[7+SampleSize] to param[6+2*SampleSize]: beta
# param[7+2*SampleSize] to param[6+3*SampleSize]: sdb 

# Likelihood
# Likelihood Lvl 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p,Self)
# Likelihood Lvl 2: p(beta_p|everything else)= p(beta_p|alpha0,alpha1,sda,Percentile)
#                   p(baseline_p|everything else) = p(beta_p|alpha0,alpha1,sda,Percentile)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)p(baseline_p|r0,r1,sdr)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # predicted baseline for all subjects
  pred_baseline = r0+r1*data[1:SampleSize,]$Percentile # predicted baseline for p from level 2.
  # baseline_p likelihood
  likelihood_baseline = dnorm(baseline,mean=pred_baseline,sd=sdr, log=T)
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta)+sum(likelihood_baseline))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  r0prior = dnorm(alpha0, mean=0.25, sd = 0.2, log = T)
  r1prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  sdrprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0, sd=0.4, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+r0prior+r1prior+sdrprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(6+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,0.005,0.005,0.004,
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,6+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%100==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.3,0.1,0.5,0.3,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 1000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```


## Dataset 2: Abstract Reasoning (Raven's APM)
     
```{r, echo=FALSE, cache=TRUE}
# PART I: Data Entry
Mydataf <- read.xlsx("RavenData.xlsx",1,header=TRUE, to.data.frame = TRUE)

#----------------------------------------------------------------------#
#----------------------------------------------------------------------#

# PART II A: Data exclusion
# Correct Answers of the questionnaire
correctanswervector <- c(1,3,2,2,1,3,2,4,4,2,2,1) # correct answers

# AnswerTable:a SampleSize*12 table contains the chosen options for each quetion and each participant
answertable <- cbind(Mydataf$Q1,Mydataf$Q2,Mydataf$Q3,Mydataf$Q4,Mydataf$Q5,
                     Mydataf$Q6,Mydataf$Q7,Mydataf$Q8,Mydataf$Q9,Mydataf$Q10,
                     Mydataf$Q11,Mydataf$Q12)

# make sure the answers are in numeric format
axh <- matrix(rep(0,nrow(answertable)*ncol(answertable)),nrow=nrow(answertable),ncol=ncol(answertable))
for (i in 1: nrow(answertable)){
  for (j in 1:ncol(answertable)){
    a <- answertable[i,j]
    axh[i,j] <- as.numeric(a)
  }
}
answertable <- axh

# Belieflist
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2,Mydataf$Q1B3,Mydataf$Q1B4)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2,Mydataf$Q2B3,Mydataf$Q2B4)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2,Mydataf$Q3B3,Mydataf$Q3B4)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2,Mydataf$Q4B3,Mydataf$Q4B4)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2,Mydataf$Q5B3,Mydataf$Q5B4)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2,Mydataf$Q6B3,Mydataf$Q6B4)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2,Mydataf$Q7B3,Mydataf$Q7B4)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2,Mydataf$Q8B3,Mydataf$Q8B4)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2,Mydataf$Q9B3,Mydataf$Q9B4)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2,Mydataf$Q10B3,Mydataf$Q10B4)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2,Mydataf$Q11B3,Mydataf$Q11B4)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2,Mydataf$Q12B3,Mydataf$Q12B4)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12)

# Data exclusion: exclude inattentive participants. If the chosen choice has not the maximum probability assigned, this counts as a mismatch. If a participant has equal or more than 25% of answers appear to be mismatch, this participant is regarded as an inattentive participant, and will be excluded from analysis.
ChosenBelief<-chosenbelief(belieflist,answertable)
MaximumBelief<-maxbelief(belieflist)
exclude<-mismatch(MaximumBelief,ChosenBelief)

#----------------------------------------------------------------------#

# PART II B: Data Management and Important Variable Set up
## Mydataf after Excluding SubNum 97-101 (who failed to finish the questionnaire) and inattentional participants: 8(6),12(4),29(8),30(8),54(4),64(4)
Mydataf<-Mydataf[-exclude[[2]],]
# answertable after exlucsion:
answertable<-answertable[-exclude[[2]],]
# belieflist after exclusion: (Mydataf is overrided)
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2,Mydataf$Q1B3,Mydataf$Q1B4)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2,Mydataf$Q2B3,Mydataf$Q2B4)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2,Mydataf$Q3B3,Mydataf$Q3B4)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2,Mydataf$Q4B3,Mydataf$Q4B4)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2,Mydataf$Q5B3,Mydataf$Q5B4)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2,Mydataf$Q6B3,Mydataf$Q6B4)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2,Mydataf$Q7B3,Mydataf$Q7B4)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2,Mydataf$Q8B3,Mydataf$Q8B4)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2,Mydataf$Q9B3,Mydataf$Q9B4)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2,Mydataf$Q10B3,Mydataf$Q10B4)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2,Mydataf$Q11B3,Mydataf$Q11B4)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2,Mydataf$Q12B3,Mydataf$Q12B4)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12)

# chosenbelief after exclusion
ChosenBelief<-ChosenBelief[-exclude[[2]],]
# maximumBelief after exclusion
MaximumBelief<-MaximumBelief[-exclude[[2]],]
# transform correctness of each response into binary values where 1 means correct and 0 means wrong.
CorrectCheck<-correctcheck(answertable,correctanswervector)
# beliefs assigned to the correct choices of each question each participant
CorrectBelief<-correctbelief(belieflist,correctanswervector)
#----------------------------------------------------------------------#

# KEY VARIABLES
SampleSize<-samplesize(Mydataf) # samplesize
Accuracy<-accuracy(CorrectCheck) # accuracy
BeliefAccuracy<-beliefaccuracy(CorrectBelief) # beliefaccuracy
Confidence<-itemconfidence(ChosenBelief) # item-wise confidence
QuestionSize <- ncol(CorrectCheck) # number of questions
OverallConfidence<-Mydataf$EstimatedNumCorrect/QuestionSize # overall confidence
#----------------------------------------------------------------------#
#----------------------------------------------------------------------#
## Beliefs in Social Comparison
SocialBelief1<- cbind(Mydataf$Q1O1,Mydataf$Q1O2,Mydataf$Q1O3,Mydataf$Q1O4)/100
SocialBelief2<- cbind(Mydataf$Q2O1,Mydataf$Q2O2,Mydataf$Q2O3,Mydataf$Q2O4)/100
SocialBelief3<- cbind(Mydataf$Q3O1,Mydataf$Q3O2,Mydataf$Q3O3,Mydataf$Q3O4)/100
SocialBelief4<- cbind(Mydataf$Q4O1,Mydataf$Q4O2,Mydataf$Q4O3,Mydataf$Q4O4)/100
SocialBelief5<- cbind(Mydataf$Q5O1,Mydataf$Q5O2,Mydataf$Q5O3,Mydataf$Q5O4)/100
SocialBelief6<- cbind(Mydataf$Q6O1,Mydataf$Q6O2,Mydataf$Q6O3,Mydataf$Q6O4)/100
SocialBelief7<- cbind(Mydataf$Q7O1,Mydataf$Q7O2,Mydataf$Q7O3,Mydataf$Q7O4)/100
SocialBelief8<- cbind(Mydataf$Q8O1,Mydataf$Q8O2,Mydataf$Q8O3,Mydataf$Q8O4)/100
SocialBelief9<- cbind(Mydataf$Q9O1,Mydataf$Q9O2,Mydataf$Q9O3,Mydataf$Q9O4)/100
SocialBelief10<- cbind(Mydataf$Q10O1,Mydataf$Q10O2,Mydataf$Q10O3,Mydataf$Q10O4)/100
SocialBelief11<- cbind(Mydataf$Q11O1,Mydataf$Q11O2,Mydataf$Q11O3,Mydataf$Q11O4)/100
SocialBelief12<- cbind(Mydataf$Q12O1,Mydataf$Q12O2,Mydataf$Q12O3,Mydataf$Q12O4)/100
socialbelieflist<-list(SocialBelief1,SocialBelief2,SocialBelief3,SocialBelief4,SocialBelief5,SocialBelief6,SocialBelief7,SocialBelief8,SocialBelief9,SocialBelief10,SocialBelief11,SocialBelief12)

# beliefs about other people choosing the correct choice for each question each participant
SocialCorrectBelief<-correctbelief(socialbelieflist,correctanswervector)
# beliefs about other people choosing the choice the participant chose for each question each participant
SocialChosenBelief<-chosenbelief(socialbelieflist,answertable)
# key variables
AccuracyPercentile<-accuracypercentile(Accuracy) # actual performance percentile by binary score
BeliefaccuracyPercentile<-beliefaccuracypercentile(BeliefAccuracy) # actual performance percentile by probabilistic score
SharedBelief <- beliefaccuracy(SocialChosenBelief) # mean beliefs in other people choosing the same answer as I did
SpecificPercentile<-Mydataf$SpecificPercentile/100 # estimate of one's performance relative to others in percentile on the specific questionnaire

```

```{r}
#create data array
data.array = array(dim=c(QuestionSize,3,SampleSize))
for (sub in 1: SampleSize){
  data.array[,,sub]=cbind(SocialChosenBelief[sub,],ChosenBelief[sub,],rep(SpecificPercentile[sub],QuestionSize))
}
# name the array
dimnames(data.array) <- list(1:QuestionSize, c("Other", "Self", "Percentile"), 1:SampleSize)

excludelist <- c()
# exclude inattentive participants whose answers are all the same
for (sub in 1: SampleSize) {
  x=data.array[,,sub][,"Self"]
  y=data.array[,,sub][,"Other"]
  if(length(unique(x))==1||length(unique(y))==1||cor.test(x,y)[["estimate"]]>0.999){
    excludelist = c(excludelist,sub)
  }
}
# excluded data array
data.array<-data.array[,,-excludelist]
SampleSize <- dim(data.array)[3]

data.array.tm <- NULL
for(sub in 1:SampleSize){
  data.array.tm <- rbind(data.array.tm,cbind(data.array[,,sub],Subject=rep(sub,QuestionSize)))
}

# Final Data to enter the model
comb_data= data.frame(Q=rep(1:QuestionSize,SampleSize),data.array.tm)
print(paste0("Final Sample Size is: ",SampleSize))

```
          

### Model theta on Beta(slope) as well         
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda

# setting priors: (for variables after centering)
# baseline ~ N(0,0.4^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2

# together we have a total of 3+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4] to param[3+SampleSize]: baseline
# param[4+SampleSize] to param[3+2*SampleSize]: beta
# param[4+2*SampleSize] to param[3+3*SampleSize]: sdb 

# Likelihood
# Likelihood for Level 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p)
# Likelihood for Level 2: p(beta_p|everything else) = p(beta_p|alpha0,alpha1,sda)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(4+SampleSize):(3+2*SampleSize)] #{beta_p}, vector
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)] #{sdb_p}, vector
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)]
  beta = param[(4+SampleSize):(3+2*SampleSize)]
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)]
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0, sd=0.2, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(3+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,
                       rep(0.0045,SampleSize),
                       rep(0.0045,SampleSize),
                       rep(0.004,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,3+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%1000==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.5,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 40000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```


### Model theta on Baseline as well
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
#          baseline(p) ~ r0 + r1*Percentile(p) + N(N,sdr^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda, r0, r1, sdr

# setting priors: (for variables after centering)
# baseline ~ N(0,0.2^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# r0 ~ N(0,0.4^2)
# r1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2
# sdr ~ beta(2,5)

# together we have a total of 6+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4]: r0;
# param[5]: r1;
# param[6]: sdr;
# param[7] to param[6+SampleSize]: baseline
# param[7+SampleSize] to param[6+2*SampleSize]: beta
# param[7+2*SampleSize] to param[6+3*SampleSize]: sdb 

# Likelihood
# Likelihood Lvl 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p,Self)
# Likelihood Lvl 2: p(beta_p|everything else)= p(beta_p|alpha0,alpha1,sda,Percentile)
#                   p(baseline_p|everything else) = p(beta_p|alpha0,alpha1,sda,Percentile)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)p(baseline_p|r0,r1,sdr)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # predicted baseline for all subjects
  pred_baseline = r0+r1*data[1:SampleSize,]$Percentile # predicted baseline for p from level 2.
  # baseline_p likelihood
  likelihood_baseline = dnorm(baseline,mean=pred_baseline,sd=sdr, log=T)
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta)+sum(likelihood_baseline))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  r0prior = dnorm(alpha0, mean=0.25, sd = 0.2, log = T)
  r1prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  sdrprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0, sd=0.4, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+r0prior+r1prior+sdrprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(6+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,0.005,0.005,0.004,
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,6+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%100==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.3,0.1,0.5,0.3,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 1000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```

        
## Dataset 3: Global Literacy 
     
```{r, echo=FALSE, cache=TRUE}
# Part I: Data Entry
# read data
Mydataf <- read.xlsx("GlobalLiteracy_clean.xlsx",1,header=TRUE, to.data.frame = TRUE)

#----------------------------------------------------------------------#
#----------------------------------------------------------------------#

# PART II A: Data exclusion
# Correct Answers of the questionnaire
correctanswervector <- c(3,2,3,4,2,1,4,2,1,2,1,1,4,2,2,2) # correct answers

# AnswerTable:a SampleSize*16 table contains the chosen options for each quetions and each participant
answertable <- cbind(Mydataf$Q1,Mydataf$Q2,Mydataf$Q3,Mydataf$Q4,
                     Mydataf$Q5,Mydataf$Q6,Mydataf$Q7,Mydataf$Q8,
                     Mydataf$Q9,Mydataf$Q10,Mydataf$Q11,Mydataf$Q12,
                     Mydataf$Q13,Mydataf$Q14,Mydataf$Q15,Mydataf$Q16)

# make sure the answers are in numeric format
axh <- matrix(rep(0,nrow(answertable)*ncol(answertable)),nrow=nrow(answertable),ncol=ncol(answertable))
for (i in 1: nrow(answertable)){
  for (j in 1:ncol(answertable)){
    a <- answertable[i,j]
    axh[i,j] <- as.numeric(a)
  }
}
answertable <- axh

# Belieflist
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2,Mydataf$Q1B3,Mydataf$Q1B4)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2,Mydataf$Q2B3,Mydataf$Q2B4)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2,Mydataf$Q3B3,Mydataf$Q3B4)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2,Mydataf$Q4B3,Mydataf$Q4B4)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2,Mydataf$Q5B3,Mydataf$Q5B4)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2,Mydataf$Q6B3,Mydataf$Q6B4)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2,Mydataf$Q7B3,Mydataf$Q7B4)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2,Mydataf$Q8B3,Mydataf$Q8B4)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2,Mydataf$Q9B3,Mydataf$Q9B4)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2,Mydataf$Q10B3,Mydataf$Q10B4)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2,Mydataf$Q11B3,Mydataf$Q11B4)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2,Mydataf$Q12B3,Mydataf$Q12B4)/100
Belief13<- cbind(Mydataf$Q13B1,Mydataf$Q13B2,Mydataf$Q13B3,Mydataf$Q13B4)/100
Belief14<- cbind(Mydataf$Q14B1,Mydataf$Q14B2,Mydataf$Q14B3,Mydataf$Q14B4)/100
Belief15<- cbind(Mydataf$Q15B1,Mydataf$Q15B2,Mydataf$Q15B3,Mydataf$Q15B4)/100
Belief16<- cbind(Mydataf$Q16B1,Mydataf$Q16B2,Mydataf$Q16B3,Mydataf$Q16B4)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12,Belief13,Belief14,Belief15,Belief16)

# Data exclusion: exclude inattentive participants. If the chosen choice has not the maximum probability assigned, this counts as a mismatch. If a participant has equal or more than 25% of answers appear to be mismatch, this participant is regarded as an inattentive participant, and will be excluded from analysis.
ChosenBelief<-chosenbelief(belieflist,answertable)
MaximumBelief<-maxbelief(belieflist)
exclude<-mismatch(MaximumBelief,ChosenBelief)

#----------------------------------------------------------------------#

# PART II B: Data Management and Important Variable Set up
## Mydataf after Excluding SubNum 97-101 (who failed to finish the questionnaire) and inattentional participants: 8(6),12(4),29(8),30(8),54(4),64(4)
Mydataf<-Mydataf[-exclude[[2]],]
# answertable after exclusion:
answertable<-answertable[-exclude[[2]],]
# belieflist after exclusion: (Mydataf is overrode)
Belief1<- cbind(Mydataf$Q1B1,Mydataf$Q1B2,Mydataf$Q1B3,Mydataf$Q1B4)/100
Belief2<- cbind(Mydataf$Q2B1,Mydataf$Q2B2,Mydataf$Q2B3,Mydataf$Q2B4)/100
Belief3<- cbind(Mydataf$Q3B1,Mydataf$Q3B2,Mydataf$Q3B3,Mydataf$Q3B4)/100
Belief4<- cbind(Mydataf$Q4B1,Mydataf$Q4B2,Mydataf$Q4B3,Mydataf$Q4B4)/100
Belief5<- cbind(Mydataf$Q5B1,Mydataf$Q5B2,Mydataf$Q5B3,Mydataf$Q5B4)/100
Belief6<- cbind(Mydataf$Q6B1,Mydataf$Q6B2,Mydataf$Q6B3,Mydataf$Q6B4)/100
Belief7<- cbind(Mydataf$Q7B1,Mydataf$Q7B2,Mydataf$Q7B3,Mydataf$Q7B4)/100
Belief8<- cbind(Mydataf$Q8B1,Mydataf$Q8B2,Mydataf$Q8B3,Mydataf$Q8B4)/100
Belief9<- cbind(Mydataf$Q9B1,Mydataf$Q9B2,Mydataf$Q9B3,Mydataf$Q9B4)/100
Belief10<- cbind(Mydataf$Q10B1,Mydataf$Q10B2,Mydataf$Q10B3,Mydataf$Q10B4)/100
Belief11<- cbind(Mydataf$Q11B1,Mydataf$Q11B2,Mydataf$Q11B3,Mydataf$Q11B4)/100
Belief12<- cbind(Mydataf$Q12B1,Mydataf$Q12B2,Mydataf$Q12B3,Mydataf$Q12B4)/100
Belief13<- cbind(Mydataf$Q13B1,Mydataf$Q13B2,Mydataf$Q13B3,Mydataf$Q13B4)/100
Belief14<- cbind(Mydataf$Q14B1,Mydataf$Q14B2,Mydataf$Q14B3,Mydataf$Q14B4)/100
Belief15<- cbind(Mydataf$Q15B1,Mydataf$Q15B2,Mydataf$Q15B3,Mydataf$Q15B4)/100
Belief16<- cbind(Mydataf$Q16B1,Mydataf$Q16B2,Mydataf$Q16B3,Mydataf$Q16B4)/100
belieflist<-list(Belief1,Belief2,Belief3,Belief4,Belief5,Belief6,Belief7,Belief8,Belief9,Belief10,Belief11,Belief12,Belief13,Belief14,Belief15,Belief16)
# chosenbelief after exclusion
ChosenBelief<-ChosenBelief[-exclude[[2]],]
# maximumBelief after exclusion
MaximumBelief<-MaximumBelief[-exclude[[2]],]
# transform correctness of each response into binary values where 1 means correct and 0 means wrong.
CorrectCheck<-correctcheck(answertable,correctanswervector)
# beliefs assigned to the correct choices of each question each participant
CorrectBelief<-correctbelief(belieflist,correctanswervector)
#----------------------------------------------------------------------#

# KEY VARIABLES
SampleSize<-samplesize(Mydataf) # samplesize
Accuracy<-accuracy(CorrectCheck) # accuracy
BeliefAccuracy<-beliefaccuracy(CorrectBelief) # beliefaccuracy
Confidence<-itemconfidence(ChosenBelief) # item-wise confidence
QuestionSize <- ncol(CorrectCheck) # number of questions
OverallConfidence<-Mydataf$EstimatedNumCorrect/QuestionSize # overall confidence/global confidence
#----------------------------------------------------------------------#
#----------------------------------------------------------------------#
## Beliefs in Social Comparison
SocialBelief1<- cbind(Mydataf$Q1O1,Mydataf$Q1O2,Mydataf$Q1O3,Mydataf$Q1O4)/100
SocialBelief2<- cbind(Mydataf$Q2O1,Mydataf$Q2O2,Mydataf$Q2O3,Mydataf$Q2O4)/100
SocialBelief3<- cbind(Mydataf$Q3O1,Mydataf$Q3O2,Mydataf$Q3O3,Mydataf$Q3O4)/100
SocialBelief4<- cbind(Mydataf$Q4O1,Mydataf$Q4O2,Mydataf$Q4O3,Mydataf$Q4O4)/100
SocialBelief5<- cbind(Mydataf$Q5O1,Mydataf$Q5O2,Mydataf$Q5O3,Mydataf$Q5O4)/100
SocialBelief6<- cbind(Mydataf$Q6O1,Mydataf$Q6O2,Mydataf$Q6O3,Mydataf$Q6O4)/100
SocialBelief7<- cbind(Mydataf$Q7O1,Mydataf$Q7O2,Mydataf$Q7O3,Mydataf$Q7O4)/100
SocialBelief8<- cbind(Mydataf$Q8O1,Mydataf$Q8O2,Mydataf$Q8O3,Mydataf$Q8O4)/100
SocialBelief9<- cbind(Mydataf$Q9O1,Mydataf$Q9O2,Mydataf$Q9O3,Mydataf$Q9O4)/100
SocialBelief10<- cbind(Mydataf$Q10O1,Mydataf$Q10O2,Mydataf$Q10O3,Mydataf$Q10O4)/100
SocialBelief11<- cbind(Mydataf$Q11O1,Mydataf$Q11O2,Mydataf$Q11O3,Mydataf$Q11O4)/100
SocialBelief12<- cbind(Mydataf$Q12O1,Mydataf$Q12O2,Mydataf$Q12O3,Mydataf$Q12O4)/100
SocialBelief13<- cbind(Mydataf$Q13O1,Mydataf$Q13O2,Mydataf$Q13O3,Mydataf$Q13O4)/100
SocialBelief14<- cbind(Mydataf$Q14O1,Mydataf$Q14O2,Mydataf$Q14O3,Mydataf$Q14O4)/100
SocialBelief15<- cbind(Mydataf$Q15O1,Mydataf$Q15O2,Mydataf$Q15O3,Mydataf$Q15O4)/100
SocialBelief16<- cbind(Mydataf$Q16O1,Mydataf$Q16O2,Mydataf$Q16O3,Mydataf$Q16O4)/100
socialbelieflist<-list(SocialBelief1,SocialBelief2,SocialBelief3,SocialBelief4,SocialBelief5,SocialBelief6,SocialBelief7,SocialBelief8,SocialBelief9,SocialBelief10,SocialBelief11,SocialBelief12,SocialBelief13,SocialBelief14,SocialBelief15,SocialBelief16)

# beliefs about other people choosing the correct choice for each question each participant
SocialCorrectBelief<-correctbelief(socialbelieflist,correctanswervector)
# beliefs about other people choosing the choice the participant chose for each question each participant
SocialChosenBelief<-chosenbelief(socialbelieflist,answertable)
# key variables
AccuracyPercentile<-accuracypercentile(Accuracy) # actual performance percentile by binary score
BeliefaccuracyPercentile<-beliefaccuracypercentile(BeliefAccuracy) # actual performance percentile by probabilistic score
SharedBelief <- beliefaccuracy(SocialChosenBelief) # mean beliefs in other people choosing the same answer as I did
SpecificPercentile<-Mydataf$SpecificPercentile/100 # estimate of one's performance relative to others in percentile on the specific questionnaire
```

```{r}
#create data array
data.array = array(dim=c(QuestionSize,3,SampleSize))
for (sub in 1: SampleSize){
  data.array[,,sub]=cbind(SocialChosenBelief[sub,],ChosenBelief[sub,],rep(SpecificPercentile[sub],QuestionSize))
}
# name the array
dimnames(data.array) <- list(1:QuestionSize, c("Other", "Self", "Percentile"), 1:SampleSize)

excludelist <- c()
# exclude inattentive participants whose answers are all the same
for (sub in 1: SampleSize) {
  x=data.array[,,sub][,"Self"]
  y=data.array[,,sub][,"Other"]
  if(length(unique(x))==1||length(unique(y))==1||cor.test(x,y)[["estimate"]]>0.999){
    excludelist = c(excludelist,sub)
  }
}
# excluded data array
data.array<-data.array[,,-excludelist]
SampleSize <- dim(data.array)[3]

data.array.tm <- NULL
for(sub in 1:SampleSize){
  data.array.tm <- rbind(data.array.tm,cbind(data.array[,,sub],Subject=rep(sub,QuestionSize)))
}

# Final Data to enter the model
comb_data= data.frame(Q=rep(1:QuestionSize,SampleSize),data.array.tm)
print(paste0("Final Sample Size is: ",SampleSize))

```
          

### Model theta on Beta(slope) as well         
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda

# setting priors: (for variables after centering)
# baseline ~ N(0.25,0.2^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2

# together we have a total of 3+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4] to param[3+SampleSize]: baseline
# param[4+SampleSize] to param[3+2*SampleSize]: beta
# param[4+2*SampleSize] to param[3+3*SampleSize]: sdb 

# Likelihood
# Likelihood for Level 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p)
# Likelihood for Level 2: p(beta_p|everything else) = p(beta_p|alpha0,alpha1,sda)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(4+SampleSize):(3+2*SampleSize)] #{beta_p}, vector
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)] #{sdb_p}, vector
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  baseline = param[4:(SampleSize+3)]
  beta = param[(4+SampleSize):(3+2*SampleSize)]
  sdb = param[(4+2*SampleSize):(3+3*SampleSize)]
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0.25, sd=0.2, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(3+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,
                       rep(0.0045,SampleSize),
                       rep(0.0045,SampleSize),
                       rep(0.004,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,3+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%1000==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.5,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 40000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```


### Model theta on Baseline as well
```{r}
# Multilevel Model
# Linear model: 
# Level 1: Others(pi) = baseline(p) + beta(p)*Self(pi) + N(0,sdb(p)^2) 
# Level 2: Beta(p) ~ alpha0 + alpha1*Percentile(p) + N(0,sda^2)
#          baseline(p) ~ r0 + r1*Percentile(p) + N(N,sdr^2)
 
# Where:
# p is participant index, i is question index 
# Data: Others, Self & Percentile
# parameter: 
# person-wise parameters: baseline(p), beta(p), sdb. All 3 variables have the size of sample size. 
# global parameters: alpha0, alpha1, sda, r0, r1, sdr

# setting priors: (for variables after centering)
# baseline ~ N(0.25,0.2^2) for every subject p
# beta ~ N(0,0.4^2) for every subject p
# alpha0 ~ N(0,0.4^2) # N(0,0.4^2) can nicely span [-1,1]
# alpha1 ~ N(0,0.4^2)
# r0 ~ N(0,0.4^2)
# r1 ~ N(0,0.4^2)
# sdb ~ beta(2,5) for each subject p
# sda ~ beta(2,5) where beta(2,5) have the mode=0.2
# sdr ~ beta(2,5)

# together we have a total of 6+3*SampleSize parameters

# param is a vector to store the parameters:
# param[1]: alpha0;
# param[2]: alpha1;
# param[3]: sda
# param[4]: r0;
# param[5]: r1;
# param[6]: sdr;
# param[7] to param[6+SampleSize]: baseline
# param[7+SampleSize] to param[6+2*SampleSize]: beta
# param[7+2*SampleSize] to param[6+3*SampleSize]: sdb 

# Likelihood
# Likelihood Lvl 1: p(Y_pi|everything else)=p(Y_pi|baseline_p,beta_p,sdb_p,Self)
# Likelihood Lvl 2: p(beta_p|everything else)= p(beta_p|alpha0,alpha1,sda,Percentile)
#                   p(baseline_p|everything else) = p(beta_p|alpha0,alpha1,sda,Percentile)
# aggregated: p(Y|everything else) =
# \prod_p[\prod_i[p(Y_pi|baseline_p,beta_p,sdb_p)]p(beta_p|alpha0,alpha1,sda)p(baseline_p|r0,r1,sdr)] 

# Likelihood Function
likelihood = function(param,data,SampleSize){
  data=data
  SampleSize=SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # predicted baseline for all subjects
  pred_baseline = r0+r1*data[1:SampleSize,]$Percentile # predicted baseline for p from level 2.
  # baseline_p likelihood
  likelihood_baseline = dnorm(baseline,mean=pred_baseline,sd=sdr, log=T)
  # predicted beta for all subjects
  pred_beta = alpha0+alpha1*data[1:SampleSize,]$Percentile # predicted beta for p from level 2.
  # beta_p likelihood
  likelihood_beta = dnorm(beta,mean=pred_beta,sd=sda, log=T)
  # initialize each person p's likelihood
  likelihood_p <- rep(NA,SampleSize)
  # y_p likelihood
  for (p in 1:SampleSize){
    data.sub = data[which(data$Subject==p),] # single subject p's data 
    pred_pi = baseline[p] + beta[p]*data.sub$Self # predicted Y_pi, a vector
    # Y_pi likelihood
    likelihood_pi = dnorm(data.sub$Other, mean = pred_pi, sd=sdb[p], log=T) # a vector
    # Y_p likelihood: across items and across two levels
    likelihood_p[p] = sum(likelihood_pi) # sum(log) = log(product of p(y))
  }
  # return overall likelihood
  return(sum(likelihood_p)+sum(likelihood_beta)+sum(likelihood_baseline))
}

# Prior Function
prior= function(param,SampleSize){
  SampleSize = SampleSize
  alpha0 = param[1]
  alpha1 = param[2]
  sda = param[3]
  r0 = param[4]
  r1 = param[5]
  sdr = param[6]
  baseline = param[7:(SampleSize+6)] # {baseline_p} for p=1,...,SampleSize, vector
  beta = param[(7+SampleSize):(6+2*SampleSize)] #{beta_p}, vector
  sdb = param[(7+2*SampleSize):(6+3*SampleSize)] #{sdb_p}, vector
  # prior
  alpha0prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  alpha1prior = dnorm(alpha1, mean=0, sd = 0.4, log = T)
  sdaprior = dbeta(sda, 2, 5, log = T)
  r0prior = dnorm(alpha0, mean=0.25, sd = 0.2, log = T)
  r1prior = dnorm(alpha0, mean=0, sd = 0.4, log = T)
  sdrprior = dbeta(sda, 2, 5, log = T)
  baselineprior = dnorm(baseline, mean=0.25, sd=0.2, log = T) 
  betaprior = dnorm(beta, mean=0, sd = 0.4, log = T) 
  sdbprior = dbeta(sdb, 2, 5, log = T)
  return(alpha0prior+alpha1prior+sdaprior+r0prior+r1prior+sdrprior+sum(baselineprior)+sum(betaprior)+sum(sdbprior)) 
  # sum(log) = log(product of priors)
}

# Posterior (Joint actually)
posterior = function(param,data,SampleSize){
  SampleSize=SampleSize
  data=data
  return (likelihood(param,data,SampleSize) + prior(param,SampleSize))
}

# Metropolis-Hastings MCMC

# proposal function
# We adopted "normal random walk"
proposalfunction = function(param,SampleSize){
    SampleSize = SampleSize
    return(rnorm(6+3*SampleSize, 
                 mean = param, 
                 sd= c(0.005,0.005,0.004,0.005,0.005,0.004,
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize),
                       rep(0.002,SampleSize))))
}
 
run_metropolis_MCMC = function(startvalue, iterations, data, SampleSize){
    SampleSize = SampleSize
    data=data
    chain = array(dim = c(iterations+1,6+3*SampleSize))
    chain[1,] = startvalue
    for (i in 1:iterations){
        if(i%%100==0){
          print(i)
        }
        proposal = proposalfunction(chain[i,],SampleSize)
        R = exp(posterior(proposal,data,SampleSize) - posterior(chain[i,],data,SampleSize))
        if(is.nan(R)){
          # when the proposed sd is a negative number, which is conceptually impossible, 
          # it means MCMC walk to a position that should be rejected
          # If this happens, we will get an NaN for likelihood function and NaN for R
          # so we set it as 0 to be rejected
          R=0  
        }
        if(runif(1) < R){
            chain[i+1,] = proposal
        }else{
            chain[i+1,] = chain[i,]
        }
    }
    return(chain)
}

```

```{r}
# run MCMC model
startvalue = c(0.5,0.3,0.1,0.5,0.3,0.1,rep(0,SampleSize),rep(0.2,SampleSize),rep(0.2,SampleSize))
data = comb_data
chain = run_metropolis_MCMC(startvalue, 10000, data, SampleSize)
burnIn = 1000

# check acceptance to see whether it is within 20%-30%
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```
